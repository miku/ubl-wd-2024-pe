# UBL Writing Day 2024 Prompt Engineering

> Input session on 2024-02-29, Leipzig University Library, [Martin
> Czygan](mailto:martin.czygan@gmail.com), software developer, author and data
> engineer

## About Me

* Software Developer at [Leipzig University
  Library](https://ub.uni-leipzig.de), Open Data Engineer at [Internet
Archive](https://archive.org), working on [Internet Archive
Scholar](https://en.wikipedia.org/wiki/Internet_Archive_Scholar) and [Citation Graphs](https://arxiv.org/abs/2110.06595)

[![](static/ia-scholar-hp.png)](https://scholar.archive.org)

* Misc: consultant,
  [author](https://scholar.google.com/citations?user=7gueY4EAAAAJ), open source
[contributor](https://github.com/miku), community
[organizer](https://golangleipzig.space/), former Lecturer at [Lancaster
University](https://www.lancasterleipzig.de/) Leipzig
* main "serious" topic, beside
  [haikus](https://golangleipzig.space/meetup-38-llm-haiku/meetup-38-llm-haiku.pdf),
is the conversion of unstructured data (e.g. "strings", "bytes") to structured data
(e.g. "metadata"), information retrieval
* previous talks: [NN tour](https://github.com/miku/nntour) (2016), [PyTorch tour](https://github.com/miku/pytorch-tour) (2018), [ML w/ Go](https://github.com/miku/mlgo) (2018), [cgosamples](https://github.com/miku/cgosamples) (2023), [local
  models](https://github.com/miku/localmodels) (2023)

## From black boxes to open models (and back)

> But there is frustration in the science community over OpenAI's secrecy
> around how the model was trained and what data were used, and how GPT-4
> actually works. "**All of these closed-source models, they are essentially dead
> ends in science**," says [Sasha Luccioni](https://www.sashaluccioni.com/), a research scientist specializing in
> climate at HuggingFace, an open-source AI cooperative. --
> [GPT-4 IS HERE: WHAT SCIENTISTS THINK](http://www.hajim.rochester.edu/che/assets/pdf/gpt-4-is-here-what-scientists-think.pdf) (03/2023)

* local models offer better accessibility, transparency, privacy, allow more experimentation, etc.
* NVIDIA released a local "Chat with your documents" application for Windows: [Chat with RTX](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/) on February 13, 2024

Making models and tools **open** can also be used as "marketing tactic" by
VC-funded companies to gain traction, etc.

## Prompt engineering

* the wikipedia article about [Prompt
  Engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is not that old, it first appeared in
[2021-10-20](https://en.wikipedia.org/w/index.php?title=Prompt_engineering&oldid=1050870205), a Wednesday
* I used tweet [1599971348717051904](https://twitter.com/alexandr_wang/status/1599971348717051904) as a joke on 2022-12-12 during an intro to programming CS class


## Learning or Memorization

> LLMs struggle with generalization (the only thing that actually matters),
> due to being entirely reliant on memorization -- [François Chollet](https://twitter.com/fchollet/status/1755250582334709970)

## Classic language models, writing and chat tools

In the June 1989 issue of Scientific American, on page 122-125, we find a
column, titled [A potpourri of programmed prose and
prosody](https://archive.org/details/ComputerRecreationsMarkovChainer):

[![COMPUTER RECREATIONS. Scientific American, 260(6), June 1989, 122–125](static/computer-recreations-markov-page-1-50.png)](https://archive.org/details/ComputerRecreationsMarkovChainer)

It takes about 10s on a CPU to create a language model from 400K words, using
a slightly strange combination of texts, e.g. bible, python docs, etc.

Example output:

```shell
$ python gentext.py

with likewise the itertools module the result gives total ordering its
generally true that am the lord when thy days that were numbered of them as he
goeth in to wait for enqueued tasks have completed before ascompleted is called
without calling sysexcinfo bpo allow the creation of new features related to
fifo false if pattern is relative and then sort again
```

Well. Let's add more training data. How about 224M files of text, about 40M
words: [Screenie](x/markov/641812.gif).

Other example of programmatic text generation:

* [https://kingjamesprogramming.tumblr.com/](https://kingjamesprogramming.tumblr.com/)

> Posts generated by a Markov chain trained on the King James Bible, Structure
> and Interpretation of Computer Programs, and some of Eric S. Raymond's
> writings Run by Michael Walker (barrucadu).

* [SCIgen](https://pdos.csail.mit.edu/archive/scigen/) - An Automatic CS Paper Generator (2005)
* [Eliza](https://web.stanford.edu/class/cs124/p36-weizenabaum.pdf) (1966)

